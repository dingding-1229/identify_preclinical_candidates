{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "algorithm = 'RF_corr_ratio_ap'\n",
    "\n",
    "# RF/XGB\n",
    "# corr/auto\n",
    "# threshold/ratio\n",
    "# roc/ap\n",
    "# paragrid\n",
    "\n",
    "patents_path='./patents SMILES/'\n",
    "topology_path='./topology/'\n",
    "descriptor_path='./descriptor/'\n",
    "basic_path='./basic/'\n",
    "result_path='./result/'\n",
    "\n",
    "df = pd.read_excel('drugbank_extract_filtered.xlsx')\n",
    "drug_name_list = df['Generic Name'].tolist()\n",
    "compound_num_list = df['compounds number'].tolist()\n",
    "\n",
    "drug_list = []\n",
    "for i in range(0,len(df)):\n",
    "  one = str(compound_num_list[i]) + '_' + drug_name_list[i] + '.xlsx'\n",
    "  drug_list.append(one)\n",
    "\n",
    "drug_list = sorted(drug_list, key=lambda x: int(x.split('_')[0]))\n",
    "print(len(drug_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(X,y):\n",
    "    X1 = X.values\n",
    "\n",
    "    if 'RF' in algorithm:\n",
    "        model = RandomForestClassifier(random_state=1)\n",
    "        param_grid = {\n",
    "            'n_estimators':[100,200,300],\n",
    "            'class_weight':[{1:50,0:1}],\n",
    "            'max_depth':[3,6,9]\n",
    "        }\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators':[300],\n",
    "            'class_weight':[{1:50,0:1}],\n",
    "            'max_depth':[6]\n",
    "        }\n",
    "\n",
    "    if 'XGB' in algorithm:\n",
    "        model = xgb.XGBClassifier(seed=1,tree_method = \"hist\")\n",
    "        param_grid = {\n",
    "        'n_estimators':[100,200,300],\n",
    "        'scale_pos_weight':[50],\n",
    "        'max_depth':[3,6,9]\n",
    "        }\n",
    "\n",
    "        param_grid = {\n",
    "        'n_estimators':[100],\n",
    "        'scale_pos_weight':[50],\n",
    "        'max_depth':[3]\n",
    "        }\n",
    "\n",
    "    # Set up outer cross-validation loop\n",
    "    outer_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "    metrics_scores = []\n",
    "    best_params = []\n",
    "    cm_list = []\n",
    "\n",
    "    # Execute outer splits\n",
    "    for train_index, test_index in outer_cv.split(X1, y):\n",
    "        X_train, X_test = X1[train_index, :], X1[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # 数据预处理\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        if 'RF' in algorithm:\n",
    "            X_train, X_test = np.nan_to_num(X_train, nan=0), np.nan_to_num(X_test, nan=0)\n",
    "\n",
    "        # Set up inner cross-validation loop\n",
    "        inner_cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "        # Create GridSearchCV object\n",
    "        if 'roc' in algorithm:\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, scoring='roc_auc', error_score='raise',n_jobs=9)\n",
    "        if 'ap' in algorithm:\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, scoring='average_precision', error_score='raise',n_jobs=9)\n",
    "\n",
    "        # Fit GridSearchCV on the training set\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities on the test set\n",
    "        y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_list.append(cm)\n",
    "        \n",
    "        # Calculate scores for the fold\n",
    "        if 'roc' in algorithm:\n",
    "            metrics_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "        if 'ap' in algorithm:\n",
    "            metrics_score = average_precision_score(y_test, y_pred_proba[:, 1])\n",
    "        metrics_scores.append(metrics_score)\n",
    "        best_params.append(clf.best_params_)\n",
    "\n",
    "        # 排序参数重要性\n",
    "        best_model = clf.best_estimator_\n",
    "        feature_importance = best_model.feature_importances_\n",
    "        sorted_idx = feature_importance.argsort()\n",
    "        top_sorted_idx = sorted_idx[-10:]\n",
    "\n",
    "        # 绘制重要参数图表\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(top_sorted_idx)), feature_importance[top_sorted_idx], align='center')\n",
    "        plt.yticks(range(len(top_sorted_idx)), [X.columns[i] for i in top_sorted_idx])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plttitle =  algorithm + ' Top 10 features ' + str(cutoff) + ', ' +str(len(metrics_scores))\n",
    "        plt.title(plttitle)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    # Calculate average scores across all folds\n",
    "    average_metrics_score = np.mean(metrics_scores)\n",
    "\n",
    "    # Calculate std across all folds\n",
    "    std_metrics_score = np.std(metrics_scores)\n",
    "\n",
    "    # Determine the best parameters based on the highest average scores\n",
    "    best_params_overall = best_params[np.argmax(metrics_scores)]\n",
    "\n",
    "    best_cm = cm_list[np.argmax(metrics_scores)]\n",
    "    best_accuracy = best_cm[1][1]/(best_cm[1][0]+best_cm[1][1])\n",
    "    print(best_cm)\n",
    "\n",
    "    return best_params_overall, average_metrics_score, std_metrics_score, best_cm, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'corr' in algorithm:\n",
    "    # reduce_dimention = pd.DataFrame()\n",
    "    # for drug in drug_list:\n",
    "    #     one_descriptor=pd.read_excel(descriptor_path+'descriptor_'+drug)\n",
    "    #     reduce_dimention = pd.concat([reduce_dimention, one_descriptor],axis=0,ignore_index=True)\n",
    "\n",
    "    # # 降维descriptor，选择与其他特征平均相关性较低的前64个特征\n",
    "    # correlation_matrix = reduce_dimention.iloc[:, 1:].corr()\n",
    "    # avg_correlation = correlation_matrix.mean()\n",
    "    # top_features = avg_correlation.nsmallest(64).index\n",
    "\n",
    "    # # 创建DataFrame保存特征和对应的平均相关性\n",
    "    # df = pd.DataFrame({'Feature': top_features, 'Average Correlation': [avg_correlation[feature] for feature in top_features]})\n",
    "\n",
    "    # # 保存DataFrame到本地CSV文件\n",
    "    # df.to_csv('top_features_correlation.csv', index=False)\n",
    "\n",
    "    # print(\"Table saved to 'top_features_correlation.csv'.\")\n",
    "\n",
    "    top_features = ['HallKierAlpha', 'BCUT2D_CHGLO', 'BCUT2D_LOGPLOW', 'VSA_EState5',\n",
    "    'BalabanJ', 'FpDensityMorgan1', 'MinAbsEStateIndex', 'MinEStateIndex',\n",
    "    'qed', 'BCUT2D_MRLOW', 'BCUT2D_MWLOW', 'MinPartialCharge',\n",
    "    'VSA_EState7', 'FpDensityMorgan2', 'fr_aldehyde', 'fr_isocyan', 'Ipc',\n",
    "    'Kappa3', 'fr_nitroso', 'NumRadicalElectrons', 'fr_dihydropyridine',\n",
    "    'fr_hdrzine', 'fr_thiocyan', 'fr_C_S', 'FractionCSP3', 'fr_azide',\n",
    "    'fr_diazo', 'fr_azo', 'fr_barbitur', 'fr_Ar_COO', 'fr_term_acetylene',\n",
    "    'fr_SH', 'fr_quatN', 'fr_isothiocyan', 'fr_benzodiazepine',\n",
    "    'fr_epoxide', 'fr_oxazole', 'fr_phos_acid', 'fr_amidine', 'fr_hdrzone',\n",
    "    'fr_furan', 'fr_oxime', 'fr_imide', 'fr_HOCCN', 'fr_N_O', 'fr_lactam',\n",
    "    'fr_nitro', 'MaxAbsPartialCharge', 'fr_nitro_arom_nonortho',\n",
    "    'fr_thiophene', 'fr_morpholine', 'fr_nitrile', 'fr_sulfone',\n",
    "    'fr_nitro_arom', 'fr_tetrazole', 'fr_alkyl_halide', 'fr_phos_ester',\n",
    "    'BCUT2D_MWHI', 'MinAbsPartialCharge', 'fr_unbrch_alkane', 'fr_ArN',\n",
    "    'SlogP_VSA7', 'fr_allylic_oxid', 'fr_sulfonamd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'auto' in algorithm:\n",
    "\n",
    "    reduce_dimention = pd.DataFrame()\n",
    "    for drug in drug_list:\n",
    "        one_descriptor=pd.read_excel(descriptor_path+'descriptor_'+drug)\n",
    "        reduce_dimention = pd.concat([reduce_dimention, one_descriptor],axis=0,ignore_index=True)\n",
    "\n",
    "    reduce_dimention=reduce_dimention.iloc[:, 1:].values\n",
    "\n",
    "\n",
    "    reduce_dimention = np.nan_to_num(reduce_dimention, nan=0)\n",
    "    reduce_dimention = scaler.fit_transform(reduce_dimention)\n",
    "    # 构建autoencoder模型\n",
    "    input_img = Input(shape=(reduce_dimention.shape[1],))\n",
    "    encoded = Dense(128, activation='relu')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(reduce_dimention.shape[1], activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "    # 训练autoencoder\n",
    "    autoencoder.fit(reduce_dimention, reduce_dimention, epochs=500, batch_size=256, shuffle=True)\n",
    "\n",
    "    # 提取autoencoder的中间层作为特征\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # 生成特征\n",
    "    reduce_dimention_encoded = encoder.predict(reduce_dimention)\n",
    "\n",
    "    # 将 NumPy 数组转换为 DataFrame，并使用自动生成的列名\n",
    "    np_df = pd.DataFrame(reduce_dimention_encoded, columns=[f'feature_{i}' for i in range(reduce_dimention_encoded.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_thres(lst, target):\n",
    "    closest_index = None\n",
    "    min_diff = float('inf')  # 初始化最小差值为正无穷\n",
    "    \n",
    "    for i, num in enumerate(lst):\n",
    "        diff = abs(num - target)  # 计算当前元素与目标值的差值\n",
    "        \n",
    "        if diff < min_diff:  # 如果当前差值更小，则更新最小差值和对应的索引\n",
    "            min_diff = diff\n",
    "            closest_index = i\n",
    "    \n",
    "    return (closest_index + 1)/10\n",
    "\n",
    "# 选定一个ratio，然后每个药物选择最接近的threshold，做成字典，后续模型参数输入\n",
    "\n",
    "\n",
    "if 'ratio' in algorithm:\n",
    "    ratio_df = pd.read_excel('residue_ratio.xlsx')\n",
    "    ratios = [round(i * 0.1, 1) for i in range(1, 10)]\n",
    "\n",
    "    ratio_thres_df = pd.DataFrame()\n",
    "    for drug in drug_list:\n",
    "        drugname = drug.split('.')[0].split('_')[1]\n",
    "        dict1={}\n",
    "        dict1['drug_name'] = drug.split('.')[0].split('_')[1]\n",
    "        ratio_list = ratio_df[ratio_df['drug_name'] == drugname].values.tolist()[0][1:]\n",
    "        for ratio in ratios:\n",
    "            \n",
    "            pair_threshold = find_closest_thres(ratio_list, ratio)\n",
    "            dict1[ratio] = pair_threshold\n",
    "        new_row =  pd.DataFrame(dict1, index=[0])\n",
    "        ratio_thres_df = pd.concat([ratio_thres_df, new_row], ignore_index=True)\n",
    "\n",
    "    print(ratio_thres_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [round(i * 0.1, 1) for i in range(1, 10)]\n",
    "# 每一个ratio都把全部药物(可能不同threshold)加载进来，然后丢进去cv训练出一个结果\n",
    "\n",
    "result_all = pd.DataFrame()\n",
    "for cutoff in tqdm(cutoffs):\n",
    "\n",
    "    params = []\n",
    "    aver_score = []\n",
    "    std_score = []\n",
    "    cm_score = []\n",
    "    accuracy_score = []\n",
    "    \n",
    "    drug_all = pd.DataFrame()\n",
    "    for drug in drug_list:\n",
    "        basic=pd.read_excel(basic_path+'basic_'+drug)\n",
    "\n",
    "        if 'ratio' in algorithm:\n",
    "            cutoff_ = ratio_thres_df.loc[ratio_thres_df['drug_name'] == drug.split('.')[0].split('_')[1], cutoff].values[0]\n",
    "            topology=pd.read_excel(topology_path+drug.split('.')[0].split('_')[1]+'/tani_ECFP6_'+str(cutoff_)+'_'+drug)\n",
    "        if 'threshold' in algorithm:\n",
    "            topology=pd.read_excel(topology_path+drug.split('.')[0].split('_')[1]+'/tani_ECFP6_'+str(cutoff)+'_'+drug)\n",
    "\n",
    "        if 'corr' in algorithm:\n",
    "            descriptor=pd.read_excel(descriptor_path+'descriptor_'+drug)\n",
    "            drug_info = pd.merge(pd.merge(basic, topology, on='c_smiles'), descriptor, on='c_smiles')\n",
    "        if 'auto' in algorithm:\n",
    "            drug_info = pd.merge(basic, topology, on='c_smiles')\n",
    "            \n",
    "        drug_all = pd.concat([drug_all, drug_info],axis=0,ignore_index=True)\n",
    "    \n",
    "    if 'corr' in algorithm:\n",
    "        drug_all = pd.concat([drug_all.iloc[:, :14], drug_all[top_features]],axis=1)\n",
    "    if 'auto' in algorithm:\n",
    "        drug_all = pd.concat([drug_all.iloc[:, :14], np_df],axis=1)\n",
    "        \n",
    "    drug_all.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # 查看数据\n",
    "    # print(drug_all)\n",
    "    # print(drug_all['Y'].value_counts())\n",
    "\n",
    "    X = drug_all.iloc[:,6:]\n",
    "    y = drug_all.iloc[:,5]\n",
    "\n",
    "    results = nested_cv(X,y)\n",
    "\n",
    "    # Extract and store the relevant information from the result\n",
    "\n",
    "    params.append(results[0])\n",
    "    aver_score.append(results[1])\n",
    "    std_score.append(results[2])\n",
    "    cm_score.append(results[3])\n",
    "    accuracy_score.append(results[4])\n",
    "\n",
    "    # Process and save the data\n",
    "    data = {\n",
    "        'network': cutoff,\n",
    "        'params': params,\n",
    "        'metrics_aver_score': aver_score,\n",
    "        'metrics_std_score': std_score,\n",
    "        'cm': cm_score,\n",
    "        'accuracy':accuracy_score\n",
    "    }\n",
    "    result_all = pd.concat([result_all, pd.DataFrame(data)],axis=0,ignore_index=True)\n",
    "\n",
    "timenow = time.strftime(\"%m%d%H%M_\", time.localtime())\n",
    "result_all.to_excel(result_path+timenow+algorithm+'.xlsx',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
